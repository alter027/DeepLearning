# -*- coding: utf-8 -*-
"""Lstm_build_cell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E_hLHq8NjH98COcaZB2vhmTpbYxgAZb-

### initial
"""

import sys
sys.version
!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl 
!pip3 install torchvision

"""### Import and Dataset"""

import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable

# Hyper Parameters
sequence_length = 28  # 序列长度，将图像的每一列作为一个序列
input_size      = 28  # 输入数据的维度
hidden_size     = 64  # 隐藏层的size
num_layers      = 2  # 有多少层

num_classes     = 10
batch_size      = 64
num_epochs      = 1
learning_rate   = 0.01

train_dataset = dsets.MNIST(root='./data',
                           train=True,
                           transform=transforms.ToTensor(),
                           download=True)
test_dataset = dsets.MNIST(root='./data',
                           train=False,
                           transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)

"""### LSTM"""

class LSTMCELL(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.i2h = nn.Linear(input_size, 4*hidden_size).cuda()
        self.h2h = nn.Linear(hidden_size, 4*hidden_size).cuda()
    def forward(self, input, c, h):
        preactivations = self.i2h(input)
        if h is not None:
            preactivations += self.h2h(h)
        gate = [] # input, output, forget, transform
        for i in range(3):
            gate.append(torch.sigmoid(preactivations[:, i*hidden_size:(i+1)*hidden_size]).cuda())
        gate.append(torch.tanh(preactivations[:, 3*hidden_size:]).cuda())
        memunit = gate[0]*gate[3] 
        if c is not None:
            memunit += gate[2]*c 
        hidunit = gate[1]*torch.tanh(memunit)
        return hidunit, memunit

class LSTM(nn.Module):
	def __init__(self, input_size, hidden_size, num_layers):
		super(LSTM,self).__init__()
		self.CELLs = [LSTMCELL(input_size, hidden_size) for i in range(input_size)]
	def forward(self, input, c, h):
		result = []
		for i, CELL in enumerate(self.CELLs):
			next_h, next_c = CELL(input[:,i,:], c, h)
			result.append(next_h)
			c, h = next_c, next_h
		return result

"""### RNN"""

# RNN Model (Many-to-One)
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = LSTM(input_size, hidden_size, num_layers)
        self.fc = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        out = self.lstm(x,None,None)
        out = self.fc(out[-1])
        return out
      
rnn = RNN(input_size, hidden_size, num_layers)
use_cuda = True
if use_cuda and torch.cuda.is_available():
    print("HERE!")
    rnn.cuda()
else:
    print("Oops...")
    rnn()
    
# Loss and Optimizer
params = [{'params':rnn.parameters()}]
for i in rnn.lstm.CELLs:
	params.append({'params':i.parameters()})
optimizer = torch.optim.Adam(params,lr=learning_rate)
# optimizer = torch.optim.Adam(rnn.parameters(),lr=learning_rate)
criterion = nn.CrossEntropyLoss()

"""### Training and Testing"""

# Train the Model
for epoch in range(20):
    for i, (images, labels) in enumerate(train_loader):
        images = Variable(images.view(-1, sequence_length, input_size)).cuda()
        labels = Variable(labels).cuda()
        
        # Forward + Backward + Optimize
        optimizer.zero_grad()
        outputs = rnn(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
    correct, total = 0, 0
    for images, labels in test_loader:
        images = Variable(images.view(-1, sequence_length, input_size)).cuda()
        outputs = rnn(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted.cpu() == labels).sum()
    print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Accuracy: %d %%' 
#            %(epoch+1, num_epochs, i+1, len(train_loader), loss.data[0], (100 * correct / total)))

# Testing
correct, total = 0, 0
for images, labels in test_loader:
    images = Variable(images.view(-1, sequence_length, input_size)).cuda()
    outputs = rnn(images)
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted.cpu() == labels).sum()
print('Accuracy: %d %%' % (100 * correct / total))
